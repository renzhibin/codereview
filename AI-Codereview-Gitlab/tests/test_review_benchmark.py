#!/usr/bin/env python3
"""
CRBench - ä»£ç è¯„å®¡åŸºå‡†æµ‹è¯•æ¡†æ¶ (Code Review Benchmark Framework)

åŠŸèƒ½æ¦‚è¿°:
1. è‡ªåŠ¨åŒ–åŠ è½½æµ‹è¯•ç”¨ä¾‹ (Qxx_ç»´åº¦/case_xxx)
2. è°ƒç”¨ä»£ç è¯„å®¡ç³»ç»Ÿ (CodeReviewer) è¿›è¡Œè¯„å®¡
3. è§£æè¯„å®¡ç»“æœ (JSON/æ–‡æœ¬) å¹¶ä¸é¢„æœŸç»“æœ (metadata.json) è¿›è¡Œæ¯”å¯¹
4. è®¡ç®—å¤šç»´åº¦æŒ‡æ ‡: å‡†ç¡®ç‡(Precision), å¬å›ç‡(Recall), F1åˆ†æ•°
5. ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š (JSON + æ§åˆ¶å°æ‘˜è¦)

æ ¸å¿ƒç‰¹æ€§:
- æ”¯æŒå¹¶å‘æµ‹è¯• (ThreadPoolExecutor)
- å…¼å®¹ JSON å’Œ æ–‡æœ¬ ä¸¤ç§è¯„å®¡ç»“æœæ ¼å¼
- æ”¯æŒæ¨¡ç³ŠåŒ¹é…å’Œå…³é”®è¯åŒ¹é… (è§£å†³ LLM è¾“å‡ºä¸ç¡®å®šæ€§é—®é¢˜)
- è¯¦ç»†çš„æŒ‡æ ‡ç»Ÿè®¡ (æŒ‰ç»´åº¦ã€æŒ‰å®‰å…¨/é€šç”¨åˆ†ç±»)

æ—¥æœŸ: 2025-12-30
"""

import os
import sys
import json
import time
import argparse
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# ============================================================================
# ç¯å¢ƒä¸è·¯å¾„é…ç½®
# ============================================================================

# è·å–é¡¹ç›®æ ¹ç›®å½• (å‡è®¾å½“å‰æ–‡ä»¶åœ¨ tests/ ç›®å½•ä¸‹ï¼Œå‘ä¸Šä¸¤çº§ä¸ºé¡¹ç›®æ ¹ç›®å½•)
PROJECT_ROOT = Path(__file__).parent.parent
# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼Œä»¥ä¾¿å¯¼å…¥ biz æ¨¡å—
sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥åŸºå‡†æµ‹è¯•ä¸“ç”¨é…ç½®
from benchmark_config import get_env_config, REVIEW_MODEL, MAX_WORKERS

# åŠ è½½å¹¶åº”ç”¨ç¯å¢ƒå˜é‡é…ç½®
ENV_CONFIG = get_env_config()
print(f"âœ… å·²åŠ è½½é…ç½® (æ¨¡å‹: {REVIEW_MODEL}, å¹¶å‘æ•°: {MAX_WORKERS})")

# åŸºå‡†æµ‹è¯•æ ¹ç›®å½• (tests/ ç›®å½•)
BENCHMARK_DIR = Path(__file__).parent

# è®¾ç½®æ—¥å¿—ç›®å½•å’Œæ–‡ä»¶è·¯å¾„
log_dir = BENCHMARK_DIR / "log"
log_dir.mkdir(exist_ok=True)
os.environ["LOG_FILE"] = str(log_dir / "test_benchmark.log")

# å°†é…ç½®æ³¨å…¥ç¯å¢ƒå˜é‡ï¼Œä¾› CodeReviewer ä½¿ç”¨
for key, value in ENV_CONFIG.items():
    os.environ[key] = value

# å°è¯•å¯¼å…¥ä¸šåŠ¡ä»£ç 
try:
    from biz.utils.code_reviewer import CodeReviewer
    from biz.service.merge_service import MergeService
    from biz.utils.log import logger
except ImportError as e:
    print(f"âŒ ä¸¥é‡é”™è¯¯: æ— æ³•å¯¼å…¥CodeRevieweræ¨¡å—: {e}")
    print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„é¡¹ç›®ç¯å¢ƒä¸‹è¿è¡Œï¼Œä¸”ä¾èµ–å·²å®‰è£…")
    sys.exit(1)


# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰ (Data Structures)
# ============================================================================

@dataclass
class TestCase:
    """
    æµ‹è¯•ç”¨ä¾‹å®ä½“ç±»
    å¯¹åº” tests/Qxx_ç»´åº¦/case_xxx ç›®å½•ä¸‹çš„ä¸€ä¸ªæµ‹è¯•åœºæ™¯
    """
    case_id: str          # ç”¨ä¾‹å”¯ä¸€æ ‡è¯†ï¼Œå¦‚ "case001_NPE"
    case_name: str        # ç”¨ä¾‹ç›®å½•å
    dimension: str        # æ‰€å±ç»´åº¦ï¼Œå¦‚ "Q01_Functionality"
    rule_id: str          # å…³è”çš„è§„åˆ™ID (å¯é€‰)ï¼Œå¦‚ "M001"
    severity: str         # ä¸¥é‡ç¨‹åº¦: critical, high, medium, low
    
    # æ–‡ä»¶è·¯å¾„
    before_file: str      # ä¿®æ”¹å‰çš„ä»£ç æ–‡ä»¶è·¯å¾„ (å¯é€‰)
    after_file: str       # ä¿®æ”¹åçš„ä»£ç æ–‡ä»¶è·¯å¾„ (ç”¨äºä¸Šä¸‹æ–‡)
    diff_file: str        # diffè¡¥ä¸æ–‡ä»¶è·¯å¾„ (æ ¸å¿ƒè¾“å…¥)
    commit_msg_file: str  # commit messageæ–‡ä»¶è·¯å¾„ (ä¸Šä¸‹æ–‡)
    metadata_file: str    # å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    
    # é¢„æœŸç»“æœ
    expected: Dict        # åŒ…å« should_detect, target_issues ç­‰é¢„æœŸä¿¡æ¯


@dataclass
class TestResult:
    """
    å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„æ‰§è¡Œç»“æœ
    åŒ…å«å®é™…è¯„å®¡ç»“æœä¸é¢„æœŸç»“æœçš„å¯¹æ¯”è¯¦æƒ…
    """
    case_id: str
    case_name: str
    dimension: str
    
    # æ ¸å¿ƒæ£€æµ‹ç»“æœ
    detected: bool                    # å®é™…æ˜¯å¦æ£€æµ‹åˆ°äº†é—®é¢˜
    should_detect: bool               # é¢„æœŸæ˜¯å¦åº”è¯¥æ£€æµ‹åˆ°é—®é¢˜
    
    # è¯¦ç»†ä¿¡æ¯
    violations: List[str]             # å®é™…æ£€æµ‹åˆ°çš„è¿è§„ç±»å‹åˆ—è¡¨ (åŒ¹é…åçš„)
    expected_issues: List[str]        # é¢„æœŸåŒ…å«çš„è¿è§„ç±»å‹åˆ—è¡¨
    
    # è¯„åˆ†ä¿¡æ¯
    score: Optional[int]              # LLM ç»™å‡ºçš„ä»£ç è¯„åˆ† (0-10)
    
    # åˆ¤å®šçŸ©é˜µ (Confusion Matrix Elements)
    is_correct: bool                  # æ€»ä½“åˆ¤å®šæ˜¯å¦æ­£ç¡® (detected == should_detect)
    is_tp: bool                       # True Positive (æ­£ä¾‹é¢„æµ‹æ­£ç¡®): æœ‰é—®é¢˜ä¸”æµ‹å‡ºäº†é—®é¢˜
    is_fp: bool                       # False Positive (å‡é˜³æ€§/è¯¯æŠ¥): æ²¡é—®é¢˜å´æµ‹å‡ºäº†é—®é¢˜
    is_fn: bool                       # False Negative (å‡é˜´æ€§/æ¼æŠ¥): æœ‰é—®é¢˜å´æ²¡æµ‹å‡ºé—®é¢˜
    is_tn: bool = False               # True Negative (çœŸé˜´æ€§): æ²¡é—®é¢˜ä¸”ç¡®å®æ²¡æµ‹å‡ºé—®é¢˜
    
    # æ€§èƒ½æŒ‡æ ‡
    elapsed_time: float = 0.0         # è€—æ—¶ (ç§’)
    
    # è°ƒè¯•ä¿¡æ¯
    raw_output: str = ""              # LLM çš„åŸå§‹è¾“å‡ºå†…å®¹ (æˆªæ–­)


@dataclass
class BenchmarkReport:
    """
    åŸºå‡†æµ‹è¯•æœ€ç»ˆæŠ¥å‘Š
    åŒ…å«æ‰€æœ‰æ±‡æ€»æŒ‡æ ‡å’Œè¯¦ç»†ç»“æœ
    """
    test_run: Dict                    # è¿è¡Œå…ƒæ•°æ® (æ—¶é—´, æ¨¡å‹, æ€»æ•°)
    overall_metrics: Dict             # æ•´ä½“æŒ‡æ ‡ (P, R, F1, Acc)
    by_dimension: Dict                # æŒ‰ç»´åº¦ç»Ÿè®¡çš„æŒ‡æ ‡
    general_review: Dict              # é€šç”¨è¯„å®¡æŒ‡æ ‡ (Q01-Q05)
    security_review: Dict             # å®‰å…¨è¯„å®¡æŒ‡æ ‡ (Q06-Q07)
    failed_cases: List[Dict]          # å¤±è´¥ç”¨ä¾‹åˆ—è¡¨ (ç”¨äºå¿«é€Ÿæ’æŸ¥)
    all_results: List[Dict]           # æ‰€æœ‰è¯¦ç»†ç»“æœ


# ============================================================================
# 1. æµ‹è¯•ç”¨ä¾‹åŠ è½½å™¨ (Test Case Loader)
# ============================================================================

class TestCaseLoader:
    """
    è´Ÿè´£ä»æ–‡ä»¶ç³»ç»Ÿä¸­æ‰«æå’ŒåŠ è½½æµ‹è¯•ç”¨ä¾‹
    """
    
    def __init__(self, benchmark_dir: str):
        self.benchmark_dir = Path(benchmark_dir)
    
    def load_all_cases(self) -> List[TestCase]:
        """
        åŠ è½½æ‰€æœ‰ç»´åº¦çš„æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        æ‰«æè§„åˆ™: tests/Q*_* ç›®å½•ä¸‹çš„æ‰€æœ‰ç”¨ä¾‹
        """
        cases = []
        # éå†æ‰€æœ‰ç»´åº¦ç›®å½• (ä»¥ Q å¼€å¤´)
        for dimension_dir in sorted(self.benchmark_dir.glob("Q*_*")):
            if not dimension_dir.is_dir():
                continue
            
            # åŠ è½½è¯¥ç»´åº¦ä¸‹çš„ç”¨ä¾‹
            dimension_cases = self.load_dimension_cases(dimension_dir.name)
            cases.extend(dimension_cases)
        
        return cases
    
    def load_dimension_cases(self, dimension: str) -> List[TestCase]:
        """
        åŠ è½½æŒ‡å®šç»´åº¦ä¸‹çš„æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        æ”¯æŒçš„ç›®å½•æ¨¡å¼: case*, positive*, trap*, real_trap*
        """
        cases = []
        dimension_dir = self.benchmark_dir / dimension
        
        if not dimension_dir.exists():
            return cases
        
        # éå†æ”¯æŒçš„ç”¨ä¾‹ç›®å½•æ¨¡å¼
        # negative*: åä¾‹/è´Ÿé¢ç”¨ä¾‹ (åº”æŠ¥è­¦)
        # positive*: æ­£ä¾‹/æ­£é¢ç”¨ä¾‹ (ä¸åº”æŠ¥è­¦)
        for pattern in ["negative*", "positive*"]:
            for case_dir in sorted(dimension_dir.glob(pattern)):
                if not case_dir.is_dir():
                    continue
                
                try:
                    case = self._load_single_case(case_dir, dimension)
                    cases.append(case)
                except Exception as e:
                    print(f"è­¦å‘Š: åŠ è½½ç”¨ä¾‹å¤±è´¥ {case_dir.name}: {e}")
        
        return cases
    
    def _load_single_case(self, case_dir: Path, dimension: str) -> TestCase:
        """
        åŠ è½½å•ä¸ªæµ‹è¯•ç”¨ä¾‹ç›®å½•ï¼Œè¯»å– metadata.json å¹¶æ„å»º TestCase å¯¹è±¡
        """
        
        # 1. è¯»å– metadata.json (å¿…é¡»å­˜åœ¨)
        metadata_file = case_dir / "metadata.json"
        if not metadata_file.exists():
            raise FileNotFoundError(f"metadata.json not found in {case_dir}")
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # 2. è§£æå…ƒæ•°æ®å­—æ®µ
        case_id = config.get("case_id", case_dir.name)
        rule_id = config.get("rule_id", "")
        severity = config.get("severity", "warning")
        should_detect = config.get("should_detect", True)  # é»˜è®¤ä¸º True (åº”è¯¥æ£€æµ‹å‡ºé—®é¢˜)
        target_issues = config.get("target_issues", [])    # é¢„æœŸçš„å…·ä½“é—®é¢˜åˆ—è¡¨
        
        # 3. æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        before_file = case_dir / "before.java"
        after_file = case_dir / "after.java"
        diff_file = case_dir / "diff.patch"
        commit_msg_file = case_dir / "commit_msg.txt"
        
        if not diff_file.exists():
            raise FileNotFoundError(f"diff.patch not found")
        
        # 4. æ„å»ºé¢„æœŸç»“æœå­—å…¸
        expected = {
            "should_detect": should_detect,
            "target_issues": target_issues
        }
        
        return TestCase(
            case_id=case_id,
            case_name=case_dir.name,
            dimension=dimension,
            rule_id=rule_id,
            severity=severity,
            before_file=str(before_file),
            after_file=str(after_file),
            diff_file=str(diff_file),
            commit_msg_file=str(commit_msg_file) if commit_msg_file.exists() else "",
            metadata_file=str(metadata_file),
            expected=expected
        )


# ============================================================================
# 2. è¯„å®¡ç³»ç»Ÿé€‚é…å™¨ (Adapter)
# ============================================================================

class ReviewSystemAdapter:
    """
    é€‚é…å™¨æ¨¡å¼ï¼šè¿æ¥åŸºå‡†æµ‹è¯•æ¡†æ¶ä¸çœŸå®çš„ä¸šåŠ¡ä»£ç  (CodeReviewer)
    è´Ÿè´£è°ƒç”¨ review æ¥å£å¹¶å¯¹ç»“æœè¿›è¡Œæ ‡å‡†åŒ–è§£æ
    """
    
    def __init__(self):
        # åˆ‡æ¢å·¥ä½œç›®å½•ä»¥ç¡®ä¿èƒ½æ­£ç¡®è¯»å–é¡¹ç›®é…ç½®
        original_dir = os.getcwd()
        project_root = Path(__file__).parent.parent
        os.chdir(str(project_root))
        
        # åˆå§‹åŒ–çœŸå®çš„ MergeService å®ä¾‹
        # è¿™é‡Œçš„ review_model ç”±ç¯å¢ƒå˜é‡æ§åˆ¶
        self.merge_service = MergeService()
        self.reviewer = CodeReviewer() # Keep for static methods if needed, or remove if not used. 
        # Actually I can access static methods via class CodeReviewer directly if imported.
        print(f"âœ… å·²è¿æ¥åˆ° MergeService (æ¨¡å‹: {REVIEW_MODEL})")
        
        os.chdir(original_dir)
    
    def review(self, test_case: TestCase) -> Tuple[str, Dict]:
        """
        æ‰§è¡Œå•ä¸ªç”¨ä¾‹çš„è¯„å®¡
        
        Returns:
            Tuple[str, Dict]: (åŸå§‹è¾“å‡ºå­—ç¬¦ä¸², è§£æåçš„ç»“æ„åŒ–ç»“æœ)
        """
        
        try:
            # Step 1: è¯»å–è¾“å…¥æ–‡ä»¶å†…å®¹
            with open(test_case.diff_file, 'r', encoding='utf-8') as f:
                diff_content = f.read()
            
            commit_msg = ""
            if test_case.commit_msg_file and os.path.exists(test_case.commit_msg_file):
                with open(test_case.commit_msg_file, 'r', encoding='utf-8') as f:
                    commit_msg = f.read().strip()
            
            context_content = ""
            if os.path.exists(test_case.after_file):
                with open(test_case.after_file, 'r', encoding='utf-8') as f:
                    context_content = f.read()
            
            # Step 2: è°ƒç”¨ MergeService.review_merge_request() æ ¸å¿ƒä¸šåŠ¡æ–¹æ³•
            service_result = self.merge_service.review_merge_request(
                diff_text=diff_content,
                commits_text=commit_msg or "ä»£ç ä¿®æ”¹",
                context=context_content   # æä¾›å®Œæ•´æ–‡ä»¶å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
            )
            
            review_result = service_result.get("review_result", "")
                                                                                                                                                                                                                                  
            # Step 3: è§£æ LLM è¿”å›çš„éç»“æ„åŒ–/åŠç»“æ„åŒ–ç»“æœ
            parsed_result = self._parse_review_result(service_result, test_case)
            
            return review_result, parsed_result
        
        except Exception as e:
            logger.error(f"è¯„å®¡å¤±è´¥ {test_case.case_id}: {e}", exc_info=True)
            print(f"âŒ è¯„å®¡å¤±è´¥ {test_case.case_id}: {e}")
            return str(e), {
                "error": str(e),
                "violations": [],
                "score": None
            }
    
    def _parse_review_result(self, service_result: Dict, test_case: TestCase) -> Dict:
        """
        è§£æè¯„å®¡ç»“æœï¼Œå¹¶åŒ¹é…æœŸæœ›çš„é—®é¢˜
        """
        # 1. ä» MergeService ç»“æœä¸­è·å–
        score = service_result.get("score")
        issues = service_result.get("question_list", [])
        review_result = service_result.get("review_result", "")
        
        target_issues = test_case.expected.get("target_issues", [])
        matched_violations = []
        
        # 2. æ£€æŸ¥è§£æå‡ºçš„é—®é¢˜æ˜¯å¦å‘½ä¸­é¢„æœŸ (ç²¾ç¡®+æ¨¡ç³ŠåŒ¹é…)
        for issue_desc in issues:
            for target in target_issues:
                # ä½¿ç”¨ CodeReviewer æä¾›çš„åŒ¹é…é€»è¾‘
                if CodeReviewer.check_issue_match(issue_desc, target):
                    if target not in matched_violations:
                        matched_violations.append(target)
        
        # 3. å…œåº•ç­–ç•¥: å·²ç§»é™¤
        
        # 4. åˆ†æ•°å…œåº•
        if not score:
            # å¦‚æœå‘ç°äº†è¿è§„é¡¹ï¼Œé»˜è®¤ç»™ 3 åˆ†ï¼›å¦åˆ™ç»™ 10 åˆ†
            score = 3 if matched_violations else 10
        
        return {
            "violations": matched_violations,
            "score": score,
            "summary": review_result[:500] if review_result else "",
            "matched_count": len(matched_violations)
        }



# ============================================================================
# 3. ç»“æœè¯„ä¼°å™¨ (Evaluator)
# ============================================================================

class ResultEvaluator:
    """
    è´Ÿè´£å°†è§£æåçš„ç»“æœä¸é¢„æœŸè¿›è¡Œå¯¹æ¯”ï¼Œç”Ÿæˆæœ€ç»ˆçš„åˆ¤å®š (Pass/Fail)
    """
    
    @staticmethod
    def evaluate(test_case: TestCase, review_output: str, 
                 parsed_result: Dict, elapsed_time: float) -> TestResult:
        """
        è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹
        
        Args:
            test_case: ç”¨ä¾‹å®šä¹‰
            review_output: LLM åŸå§‹è¾“å‡º
            parsed_result: è§£æåçš„ç»“æ„åŒ–æ•°æ®
            elapsed_time: è€—æ—¶
        """
        
        # è·å–é¢„æœŸå€¼
        expected = test_case.expected
        should_detect = expected.get("should_detect", True)
        expected_issues = expected.get("target_issues", [])
        
        # è·å–å®é™…å€¼
        actual_violations = parsed_result.get("violations", [])
        score = parsed_result.get("score")
        
        # åˆæ­¥åˆ¤å®š: æ˜¯å¦æ£€æµ‹åˆ°äº†ä»»æ„é—®é¢˜
        detected = len(actual_violations) > 0
        
        # æœ€ç»ˆåˆ¤å®š (Confusion Matrix)
        is_correct = detected == should_detect
        
        is_tp = detected and should_detect          # True Positive: åº”è¯¥æŠ¥é”™ä¸”æŠ¥é”™äº† (æ­£ç¡®å¬å›)
        is_fp = detected and not should_detect      # False Positive: ä¸è¯¥æŠ¥é”™å´æŠ¥é”™äº† (è¯¯æŠ¥)
        is_fn = not detected and should_detect      # False Negative: åº”è¯¥æŠ¥é”™å´æ²¡æŠ¥é”™ (æ¼æŠ¥)
        is_tn = not detected and not should_detect  # True Negative: ä¸è¯¥æŠ¥é”™ä¸”æ²¡æŠ¥é”™ (æ­£ç¡®é€šè¿‡)
        
        return TestResult(
            case_id=test_case.case_id,
            case_name=test_case.case_name,
            dimension=test_case.dimension,
            detected=detected,
            should_detect=should_detect,
            violations=actual_violations,
            expected_issues=expected_issues,
            score=score,
            is_correct=is_correct,
            is_tp=is_tp,
            is_fp=is_fp,
            is_fn=is_fn,
            is_tn=is_tn,
            elapsed_time=elapsed_time,
            raw_output=review_output[:1000]  # åªä¿ç•™å‰1000å­—ç¬¦ä»¥èŠ‚çœç©ºé—´
        )


# ============================================================================
# 4. æŒ‡æ ‡è®¡ç®—å™¨ (Metrics)
# ============================================================================

class MetricsCalculator:
    """
    è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ (Precision, Recall, F1, Accuracy)
    """
    
    @staticmethod
    def calculate_metrics(results: List[TestResult]) -> Dict:
        """
        è®¡ç®—ä¸€ç»„ç»“æœçš„èšåˆæŒ‡æ ‡
        """
        
        if not results:
            return {
                "total_cases": 0,
                "tp": 0, "fp": 0, "fn": 0, "tn": 0,
                "precision": 0, "recall": 0, "f1_score": 0, "accuracy": 0,
                "passed": 0, "failed": 0
            }
        
        # ç»Ÿè®¡å„åˆ†ç±»æ•°é‡
        tp = sum(1 for r in results if r.is_tp)
        fp = sum(1 for r in results if r.is_fp)
        fn = sum(1 for r in results if r.is_fn)
        tn = sum(1 for r in results if r.is_tn)
        
        # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        # Precision (æŸ¥å‡†ç‡): æŠ¥å‡ºçš„é—®é¢˜ä¸­æœ‰å¤šå°‘æ˜¯çœŸé—®é¢˜? TP / (TP + FP)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        
        # Recall (æŸ¥å…¨ç‡/å¬å›ç‡): åº”è¯¥å‘ç°çš„é—®é¢˜å‘ç°äº†å¤šå°‘? TP / (TP + FN)
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # F1 Score: På’ŒRçš„è°ƒå’Œå¹³å‡æ•°
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # Accuracy (å‡†ç¡®ç‡): æ€»ä½“åˆ¤æ–­æ­£ç¡®çš„æ¯”ä¾‹
        accuracy = (tp + tn) / len(results) if len(results) > 0 else 0
        
        return {
            "total_cases": len(results),
            "tp": tp,
            "fp": fp,
            "fn": fn,
            "tn": tn,
            "precision": round(precision, 4),
            "recall": round(recall, 4),
            "f1_score": round(f1_score, 4),
            "accuracy": round(accuracy, 4),
            "passed": sum(1 for r in results if r.is_correct),
            "failed": sum(1 for r in results if not r.is_correct)
        }
    
    @staticmethod
    def calculate_by_dimension(results: List[TestResult]) -> Dict:
        """
        æŒ‰ç»´åº¦åˆ†ç»„è®¡ç®—æŒ‡æ ‡
        """
        by_dimension = defaultdict(list)
        for result in results:
            by_dimension[result.dimension].append(result)
        
        dimension_metrics = {}
        for dimension, dim_results in by_dimension.items():
            dimension_metrics[dimension] = MetricsCalculator.calculate_metrics(dim_results)
        
        return dimension_metrics
    
    @staticmethod
    def calculate_by_category(results: List[TestResult]) -> Tuple[Dict, Dict]:
        """
        æŒ‰å¤§ç±»(é€šç”¨/å®‰å…¨)åˆ†ç»„è®¡ç®—æŒ‡æ ‡
        """
        general_dimensions = ["Q01_Functionality", "Q02_Security", 
                            "Q03_BestPractices", "Q04_Performance", "Q05_CodeStyle"]
        security_dimensions = ["Q06_HorizontalPrivilege", "Q07_VerticalPrivilege"]
        
        general_results = [r for r in results if r.dimension in general_dimensions]
        security_results = [r for r in results if r.dimension in security_dimensions]
        
        general_review = MetricsCalculator.calculate_metrics(general_results)
        general_review["dimensions"] = general_dimensions
        
        security_review = MetricsCalculator.calculate_metrics(security_results)
        security_review["dimensions"] = security_dimensions
        
        return general_review, security_review


# ============================================================================
# 5. ä¸»æµ‹è¯•æ¡†æ¶ (Runner)
# ============================================================================

class BenchmarkRunner:
    """
    åŸºå‡†æµ‹è¯•æ‰§è¡Œå…¥å£ï¼Œåè°ƒå„ä¸ªç»„ä»¶
    """
    
    def __init__(self, benchmark_dir: str):
        self.benchmark_dir = benchmark_dir
        self.loader = TestCaseLoader(benchmark_dir)
        self.adapter = ReviewSystemAdapter()
        self.evaluator = ResultEvaluator()
        self.calculator = MetricsCalculator()
    
    def run_all(self, quick: bool = False, parallel: bool = True) -> BenchmarkReport:
        """
        æ‰§è¡Œå…¨é‡æµ‹è¯•
        Args:
            quick: å¿«é€Ÿæ¨¡å¼ (åªè·‘å‰5ä¸ª)
            parallel: æ˜¯å¦å¼€å¯å¹¶å‘
        """
        
        print("ğŸš€ åŠ è½½æµ‹è¯•ç”¨ä¾‹...")
        cases = self.loader.load_all_cases()
        
        if quick:
            cases = cases[:5]
            print(f"âš¡ å¿«é€Ÿæ¨¡å¼ï¼šæµ‹è¯• {len(cases)} ä¸ªç”¨ä¾‹")
        else:
            print(f"ğŸ“Š å…±åŠ è½½ {len(cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        if parallel and len(cases) > 1:
            print(f"ğŸ”€ å¹¶å‘æ¨¡å¼ï¼šä½¿ç”¨ {MAX_WORKERS} ä¸ªworker")
            results = self._run_parallel(cases)
        else:
            print("ğŸ“ ä¸²è¡Œæ¨¡å¼")
            results = self._run_serial(cases)
        
        print("\n" + "=" * 80)
        print("ğŸ“ˆ ç”ŸæˆæŠ¥å‘Š...")
        
        return self._generate_report(results)
    
    def _run_serial(self, cases: List[TestCase]) -> List[TestResult]:
        """
        ä¸²è¡Œæ‰§è¡Œæ‰€æœ‰ç”¨ä¾‹ (ç”¨äºè°ƒè¯•)
        """
        results = []
        for i, case in enumerate(cases, 1):
            print(f"\n[{i}/{len(cases)}] æµ‹è¯• {case.case_id}...")
            start_time = time.time()
            # æ ¸å¿ƒè°ƒç”¨
            raw_output, parsed_result = self.adapter.review(case)
            elapsed_time = time.time() - start_time
            
            result = self.evaluator.evaluate(case, raw_output, parsed_result, elapsed_time)
            results.append(result)
            
            status = "âœ… PASS" if result.is_correct else "âŒ FAIL"
            print(f"   {status} | æ£€æµ‹: {result.detected} | æœŸæœ›: {result.should_detect} | è€—æ—¶: {elapsed_time:.2f}s")
        return results
    
    def _run_parallel(self, cases: List[TestCase]) -> List[TestResult]:
        """
        å¹¶å‘æ‰§è¡Œæ‰€æœ‰ç”¨ä¾‹ (ç”¨äºç”Ÿäº§/æ‰¹é‡æµ‹è¯•)
        ä½¿ç”¨ ThreadPoolExecutor
        """
        results = []
        completed = 0
        total = len(cases)
        
        def test_one_case(case: TestCase) -> TestResult:
            """å•ä¸ªç”¨ä¾‹çš„æ‰§è¡Œå‡½æ•°ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­"""
            start_time = time.time()
            raw_output, parsed_result = self.adapter.review(case)
            elapsed_time = time.time() - start_time
            return self.evaluator.evaluate(case, raw_output, parsed_result, elapsed_time)
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_case = {executor.submit(test_one_case, case): case for case in cases}
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_case):
                case = future_to_case[future]
                completed += 1
                try:
                    result = future.result()
                    results.append(result)
                    status = "âœ… PASS" if result.is_correct else "âŒ FAIL"
                    print(f"[{completed}/{total}] {case.case_id}: {status} (è€—æ—¶: {result.elapsed_time:.2f}s)")
                except Exception as e:
                    print(f"[{completed}/{total}] {case.case_id}: âŒ ERROR - {e}")
        
        # ç»“æœæŒ‰ case_id æ’åºï¼Œä¿è¯è¾“å‡ºé¡ºåºä¸€è‡´
        results.sort(key=lambda r: r.case_id)
        return results
    
    def run_dimension(self, dimension: str, parallel: bool = True) -> BenchmarkReport:
        """
        åªè¿è¡ŒæŒ‡å®šç»´åº¦çš„æµ‹è¯•
        """
        print(f"ğŸš€ åŠ è½½ç»´åº¦: {dimension}")
        cases = self.loader.load_dimension_cases(dimension)
        print(f"ğŸ“Š å…±åŠ è½½ {len(cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        if parallel and len(cases) > 1:
            print(f"ğŸ”€ å¹¶å‘æ¨¡å¼ï¼šä½¿ç”¨ {MAX_WORKERS} ä¸ªworker")
            results = self._run_parallel(cases)
        else:
            print("ğŸ“ ä¸²è¡Œæ¨¡å¼")
            results = self._run_serial(cases)
            
        return self._generate_report(results)
    
    def _generate_report(self, results: List[TestResult]) -> BenchmarkReport:
        """
        æ±‡æ€»ç»“æœå¹¶ç”Ÿæˆ BenchmarkReport å¯¹è±¡
        """
        overall_metrics = self.calculator.calculate_metrics(results)
        by_dimension = self.calculator.calculate_by_dimension(results)
        general_review, security_review = self.calculator.calculate_by_category(results)
        
        # ç­›é€‰å‡ºå¤±è´¥çš„ç”¨ä¾‹ï¼Œæ–¹ä¾¿åœ¨æŠ¥å‘Šä¸­å±•ç¤º
        failed_cases = [
            {
                "case_id": r.case_id,
                "case_name": r.case_name,
                "dimension": r.dimension,
                "detected": r.detected,
                "should_detect": r.should_detect,
                "reason": "è¯¯æŠ¥ (FP)" if r.is_fp else "æ¼æŠ¥ (FN)" if r.is_fn else "æœªçŸ¥"
            }
            for r in results if not r.is_correct
        ]
        
        return BenchmarkReport(
            test_run={
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "total_cases": len(results),
                "model": REVIEW_MODEL
            },
            overall_metrics=overall_metrics,
            by_dimension=by_dimension,
            general_review=general_review,
            security_review=security_review,
            failed_cases=failed_cases,
            all_results=[asdict(r) for r in results]
        )


# ============================================================================
# 6. æŠ¥å‘Šç”Ÿæˆå™¨ (Report Output)
# ============================================================================

class ReportGenerator:
    """
    è´Ÿè´£å°† BenchmarkReport å¯¹è±¡æ ¼å¼åŒ–è¾“å‡º (Console, File)
    """
    
    @staticmethod
    def print_summary(report: BenchmarkReport):
        """æ‰“å°æ§åˆ¶å°æ‘˜è¦æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š CRBench æµ‹è¯•æŠ¥å‘Š (Summary)")
        print("=" * 80)
        
        overall = report.overall_metrics
        print(f"\nğŸ“ˆ æ•´ä½“æŒ‡æ ‡:")
        print(f"   æ€»ç”¨ä¾‹æ•°: {overall['total_cases']}")
        print(f"   é€šè¿‡: {overall['passed']} | å¤±è´¥: {overall['failed']}")
        print(f"   å‡†ç¡®ç‡ (Precision): {overall['precision']:.2%}")
        print(f"   å¬å›ç‡ (Recall):    {overall['recall']:.2%}")
        print(f"   F1åˆ†æ•°:             {overall['f1_score']:.2%}")
        print(f"   æ­£ç¡®ç‡ (Accuracy):  {overall['accuracy']:.2%}")
        
        print(f"\nğŸ“Š æŒ‰ç»´åº¦ç»Ÿè®¡:")
        for dimension, metrics in report.by_dimension.items():
            print(f"\n   {dimension}:")
            print(f"      P={metrics['precision']:.2%} | R={metrics['recall']:.2%} | F1={metrics['f1_score']:.2%}")
        
        if report.failed_cases:
            print(f"\nâŒ å¤±è´¥ç”¨ä¾‹ ({len(report.failed_cases)}):")
            for case in report.failed_cases[:10]:
                print(f"   - {case['case_id']}: {case['reason']}")
            if len(report.failed_cases) > 10:
                print(f"   ... ç­‰å…± {len(report.failed_cases)} ä¸ª")
        
        print("\n" + "=" * 80)
    
    @staticmethod
    def save_json(report: BenchmarkReport, output_file: str):
        """ä¿å­˜å®Œæ•´ JSON æŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """CLI å…¥å£"""
    parser = argparse.ArgumentParser(description='CRBench - ä»£ç è¯„å®¡åŸºå‡†æµ‹è¯•')
    parser.add_argument('--all', action='store_true', help='è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¨¡å¼ï¼šåªæµ‹è¯•5ä¸ªç”¨ä¾‹ (ç”¨äºè°ƒè¯•)')
    parser.add_argument('--dimension', type=str, action='append', help='åªæµ‹è¯•æŒ‡å®šç»´åº¦ (å¯å¤šæ¬¡ä½¿ç”¨)')
    parser.add_argument('--serial', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ (é»˜è®¤å¹¶å‘)')
    parser.add_argument('--output', type=str, help='æŒ‡å®šè¾“å‡ºJSONæŠ¥å‘Šçš„æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ– Runner
    runner = BenchmarkRunner(benchmark_dir=str(BENCHMARK_DIR))
    
    # è·¯ç”±é€»è¾‘
    if args.dimension:
        # è¿è¡ŒæŒ‡å®šç»´åº¦
        all_results = []
        for dim in args.dimension:
            report = runner.run_dimension(dim, parallel=not args.serial)
            all_results.extend(report.all_results)
        # é‡æ–°èšåˆç”Ÿæˆæ€»æŠ¥å‘Š
        results = [TestResult(**r) for r in all_results]
        report = runner._generate_report(results)
    else:
        # è¿è¡Œå…¨éƒ¨æˆ–å¿«é€Ÿæ¨¡å¼
        report = runner.run_all(quick=args.quick, parallel=not args.serial)
    
    # è¾“å‡ºç»“æœ
    ReportGenerator.print_summary(report)
    
    # ä¿å­˜ JSON
    results_dir = BENCHMARK_DIR / "results"
    results_dir.mkdir(exist_ok=True)
    output_file = args.output if args.output else results_dir / f"benchmark_{time.strftime('%Y%m%d_%H%M%S')}.json"
    ReportGenerator.save_json(report, str(output_file))


if __name__ == '__main__':
    main()
